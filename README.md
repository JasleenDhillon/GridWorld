# GridWorld
For MDP Formulation, we have in total 48 states, S=(i,j). We can perform 4 actions which are Up, Down, Left, and Right where, probability : P( s’ | a, s). The agent moves in the intended direction with a probability of 0.7, Agent moves sideways to any side with a probability of 0.15. The reward is R( s | s’ ). R goal =+1 Reward if the Agent reaches the Goal, R trap = -1 Reward if the Agent reaches the Goal, R = - 0.02 Reward for all other state transitions, Discount Factor: 1 and Learning
Rate: 0.5.
This project is an implemenatation of value iteration, policy iteration and SARSA. 
